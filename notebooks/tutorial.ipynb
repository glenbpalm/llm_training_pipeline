{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Distributed Training with Ray and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "3. [Setup](#Setup)\n",
    "4. [Data Preparation](#data-preparation)\n",
    "5. [Model Definition](#model-definition)\n",
    "6. [Distributed Training with Ray](#distributed-training-with-ray)\n",
    "7. [Monitoring and Logging](#monitoring-and-logging)\n",
    "8. [Evaluation](#evaluation)\n",
    "9. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to this tutorial on setting up distributed training using **Ray** and **PyTorch**. In this guide, we will walk through the steps to train a GPT-2 language model across multiple nodes or GPUs, leveraging the power of Ray for distributed computing.\n",
    "\n",
    "### Objectives:\n",
    "- Understand how to integrate Ray with PyTorch for distributed training.\n",
    "- Learn how to prepare data and define models compatible with distributed training.\n",
    "- Monitor training progress and log metrics using MLflow or Weights & Biases.\n",
    "- Evaluate the trained model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following installed:\n",
    "\n",
    "- Python 3.7 or higher\n",
    "- PyTorch\n",
    "- Ray\n",
    "- Transformers (Hugging Face)\n",
    "- MLflow or Weights & Biases\n",
    "- Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers ray[default] mlflow tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import transformers and tokenizer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Import Ray and related libraries\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.integration.torch import DistributedTrainableCreator\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise Ray\n",
    "\n",
    "Initialise Ray for distributed computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we'll use a small dataset for demonstration purposes. Replace this with your dataset as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (replace with your own data)\n",
    "texts = [\n",
    "    \"Once upon a time, there was a brave knight.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"In a galaxy far far away, there was a small planet.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "]\n",
    "\n",
    "# Initialise the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "# Tokenize and encode the data\n",
    "tokenized_texts = [tokenizer.encode(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_texts, tokenizer, max_length):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_texts[idx][:self.max_length]\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            attention_mask[-padding_length:] = [0] * padding_length\n",
    "\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_seq_length = 50\n",
    "batch_size = 2\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(tokenized_texts, tokenizer, max_seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2ModelWrapper(nn.Module):\n",
    "    def __init__(self, model_name='gpt2'):\n",
    "        super(GPT2ModelWrapper, self).__init__()\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2ModelWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Initialize distributed training\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "    \n",
    "    # Setup model and optimizer\n",
    "    model = GPT2ModelWrapper()\n",
    "    model.to(local_rank)\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    # Prepare data loader\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    dataset = CustomDataset(tokenized_texts, tokenizer, max_seq_length=50)\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "    data_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=sampler)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        sampler.set_epoch(epoch)\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(local_rank)\n",
    "            attention_mask = batch['attention_mask'].to(local_rank)\n",
    "            labels = batch['labels'].to(local_rank)\n",
    "            loss, _ = model(input_ids, attention_mask, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Rank {rank}, Epoch {epoch+1}, Loss: {total_loss/len(data_loader)}\")\n",
    "    \n",
    "    # Clean up\n",
    "    torch.distributed.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure and Run Training with Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration\n",
    "config = {\n",
    "    \"lr\": 5e-5,\n",
    "    \"batch_size\": 2,\n",
    "    \"epochs\": 3\n",
    "}\n",
    "\n",
    "# Create a distributed trainable\n",
    "DistributedTrainable = DistributedTrainableCreator(\n",
    "    train_model,\n",
    "    num_workers=2,  # Number of distributed workers\n",
    "    use_gpu=True,\n",
    "    num_cpus_per_worker=1,\n",
    "    backend='nccl'  # Use 'gloo' if you're training on CPU\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "analysis = tune.run(\n",
    "    DistributedTrainable,\n",
    "    config=config,\n",
    "    num_samples=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Logging\n",
    "\n",
    "For monitoring and logging, you can integrate MLflow or Weights & Biases into your training function. Here's how you can do it with MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Integrate MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "def train_model(config):\n",
    "    # Initialize MLflow\n",
    "    if int(os.environ.get('RANK', 0)) == 0:\n",
    "        mlflow.start_run()\n",
    "    \n",
    "    # Rest of the training code...\n",
    "    \n",
    "    # Log metrics\n",
    "    if int(os.environ.get('RANK', 0)) == 0:\n",
    "        mlflow.log_metric(\"loss\", total_loss/len(data_loader), step=epoch)\n",
    "    \n",
    "    # End MLflow run\n",
    "    if int(os.environ.get('RANK', 0)) == 0:\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to start the MLflow server before running the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            loss, _ = model(input_ids, attention_mask, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            total_tokens += input_ids.size(0)\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    print(f\"Evaluation Loss: {avg_loss}, Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model (assuming single GPU for evaluation)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2ModelWrapper()\n",
    "model.to(device)\n",
    "\n",
    "# Prepare data loader\n",
    "dataset = CustomDataset(tokenized_texts, tokenizer, max_length=50)\n",
    "data_loader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, data_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've covered the basics of setting up distributed training using Ray and PyTorch. We:\n",
    "\n",
    "- Prepared and tokenized text data.\n",
    "- Defined a GPT-2 model wrapper.\n",
    "- Configured distributed training with Ray.\n",
    "- Integrated MLflow for monitoring.\n",
    "- Evaluated the trained model.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "- Experiment with larger datasets and more complex models.\n",
    "- Explore hyperparameter tuning with Ray Tune.\n",
    "- Integrate Weights & Biases for advanced monitoring.\n",
    "- Deploy the trained model using Ray Serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Ray Documentation](https://docs.ray.io/)\n",
    "- [PyTorch Documentation](https://pytorch.org/)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
